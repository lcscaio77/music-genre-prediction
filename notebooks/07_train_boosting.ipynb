{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config, src\n",
    "import numpy as np\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données chargées avec succès.\n"
     ]
    }
   ],
   "source": [
    "data = src.load_data(config.PROCESSED_DATA_COMBINED_RAP_HIPHOP_FILE)\n",
    "\n",
    "X_train, X_test, y_train, y_test = src.split_data(data, target_column='music_genre')\n",
    "\n",
    "class_weight = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight = dict(zip(np.unique(y_train), class_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie on se propose d'appliquer les méthodes de classifications de types séquentielles du Boosting. Les méthodes de Boosting remontent á l'article de Friedman sur l'approximation de fonctions \"GREEDY\" par une optimisation numérique de fonctions. Divers algorithmes tels que AdaBOOST ,XGBOOST ont été developpés ces derniéres années et se sont révéles particulièrement efficaces. On utilise l' estimateur  HistGradientBoostingClassifier car il a l'avantage d'être plus rapide que GradientBoostingClassifier\\(\\).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "Gbd=HistGradientBoostingClassifier(learning_rate=0.1,max_iter=100)\n",
    "\n",
    "Gbd.fit(X_train,y_train)\n",
    "y_pred = Gbd.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réalise une GridSearch pour améliorer nos résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': np.arange(0.05, 1, 0.2),\n",
    "    'max_iter': np.arange(50,200,10)\n",
    "}\n",
    "\n",
    "\n",
    "Gbd = HistGradientBoostingClassifier()\n",
    "\n",
    "scoring = make_scorer(accuracy_score, greater_is_better=True)\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(Gbd, param_grid, cv=5, scoring=scoring)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "scores = -grid_search.cv_results_['mean_test_score']\n",
    "scores = scores.reshape(len(param_grid['max_iterations']), len(param_grid['learning_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for i, lr in enumerate(param_grid['learning_rate']):\n",
    "    plt.plot(param_grid['max_iterations'], scores[i], label=f'learning Rate: {lr}')\n",
    "\n",
    "plt.title('Grid Search Scores')\n",
    "plt.xlabel('Max Iterations')\n",
    "plt.ylabel('Negative Mean Test Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
